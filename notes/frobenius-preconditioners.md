---
title: Frobenius Approximate Inverses
categories: [math]
---

Given a matrix $$A$$, we look for its approximate inverse in the sense
of the Frobenius norm, restricted to a given matrix subspace $$E$$:

$$
\argmin{D\in E} \quad \norm{AD - I}^2_F
$$

where $$\norm{X}^2_F = \tr(X^TX)$$. The problem is a linear
least-squares that can be solved by classical methods, but closed-form
formula exist for special cases.

# Diagonal Matrices

We begin by expanding:

$$\block{AD - I}^T \block{AD-I} = D^T A^TAD - DA^T - AD - I$$

The objective function then simplifies to:

$$
\begin{align}
f(D) &= \tr(D^T A^TAD) - 2 \tr(AD)\\
&= \tr(A^TAD^2) - 2 \tr(AD) \\
&= d^T \diag(A^TA) d - 2 \diag(A)^Td
\end{align}
$$

where $$d = \diag(D)$$. The solution is simply:

$$ d = \frac{\diag(A)}{\diag(A^TA)} = \block{ \frac{A_{ii}}{ \norm{A_i}^2} }_i $$

This preconditioner is cheap to compute, and in practice gives much
better results than the diagonal preconditioner. Convergence proof for
the Jacobi iteration exist in the case of diagonally-dominant
matrices[^Tarazaga09].

# Appendix

## Sub-multiplicativity of the Frobenius norm

$$
\begin{align}
\norm{AB}^2 &= \sum_{i,j} \block{a_i^T b_j}^2 \\
&\leq \sum_{i,j}\norm{a_i}^2\norm{b_j}^2 \quad \text{(Cauchy-Schwarz)}\\
&= \sum_i \norm{a_i}^2 \sum_j \norm{b_j}^2 \\
&= \norm{A}^2 \norm{B}^2
\end{align}
$$


# References

[^Tarazaga09]: P. Tarazaga and D. Cuellar, *“Preconditioners generated by minimizing norms”,* Computers & Mathematics with Applications, vol. 57, no. 8, pp. 1305–1312, 2009.

